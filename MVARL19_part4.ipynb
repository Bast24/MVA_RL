{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsaXz-ENaLbx"
   },
   "source": [
    "# Exploration in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HHhiMrobaW8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clonage dans 'mvarl_hands_on'...\n",
      "remote: Enumerating objects: 39, done.\u001b[K\n",
      "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
      "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
      "remote: Total 39 (delta 14), reused 37 (delta 12), pack-reused 0\u001b[K\n",
      "Dépaquetage des objets: 100% (39/39), fait.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf mvarl_hands_on/\n",
    "!git clone https://github.com/rlgammazero/mvarl_hands_on.git\n",
    "!cd mvarl_hands_on/ && git fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3jpzHHRYd0pI"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 110;\n",
       "                var nbb_unformatted_code = \"import sys\\n\\nsys.path.insert(0, \\\"./mvarl_hands_on/utils\\\")\\n\\nimport os\\nimport tqdm\\nimport numpy as np\\nfrom pprint import pprint\\nimport matplotlib.pyplot as plt\\nimport math\\nfrom gridworld import GridWorldWithPits\\nfrom tqdm import tqdm\\nimport copy\";\n",
       "                var nbb_formatted_code = \"import sys\\n\\nsys.path.insert(0, \\\"./mvarl_hands_on/utils\\\")\\n\\nimport os\\nimport tqdm\\nimport numpy as np\\nfrom pprint import pprint\\nimport matplotlib.pyplot as plt\\nimport math\\nfrom gridworld import GridWorldWithPits\\nfrom tqdm import tqdm\\nimport copy\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"./mvarl_hands_on/utils\")\n",
    "\n",
    "import os\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from gridworld import GridWorldWithPits\n",
    "from tqdm import tqdm\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lQNxcIZtaSZJ"
   },
   "source": [
    "## Finite-Horizon Tabular MDPs\n",
    "We consider finite horizon problems with horizon $H$.For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
    "\n",
    "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
    "\n",
    "\n",
    "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
    "$$\n",
    "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
    "$$\n",
    "where $V_{H+1}(s) = 0$, for any $s$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Suggestion:\n",
    "- $V$ -> $(H+1, S)$-dimensional matrix\n",
    "- deterministic policy $\\pi$ -> $(H, S)$-dimensional matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Smh0opCibDY1"
   },
   "source": [
    "**Question 1:** implement backward induction for $V^\\pi$ and $V^\\star$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhLaiao3aR4N"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 88;\n",
       "                var nbb_unformatted_code = \"def evaluate_policy(P, R, H, policy):\\n    \\\"\\\"\\\"\\n        Parameters:\\n            P: transition function (S,A,S)-dim matrix\\n            R: reward function (S,A,S)-dim matrix\\n            H: horizon\\n            policy: a deterministic policy (H, S)-dim matrix\\n            \\n        Returns:\\n            The V-function of the provided policy\\n    \\\"\\\"\\\"\\n    S, A = P.shape[0], P.shape[1]\\n    V = np.zeros((H + 1, S))\\n\\n    for h in range(H - 1, -1, -1):\\n        pi_h = policy[h]\\n        V[h] = np.sum(\\n            P[np.arange(S), pi_h] * (R[np.arange(S), pi_h] + V[h + 1]), axis=1\\n        )\\n\\n    return V\\n\\n\\ndef backward_induction(P, R, H):\\n    \\\"\\\"\\\"\\n        Parameters:\\n            P: transition function (S,A,S)-dim matrix\\n            R: reward function (S,A,S)-dim matrix\\n            H: horizon\\n            \\n        Returns:\\n            The optimal V-function\\n            The optimal policy\\n    \\\"\\\"\\\"\\n    S, A = P.shape[0], P.shape[1]\\n    V = np.zeros((H + 1, S))\\n    policy = np.zeros((H, S), dtype=int)\\n\\n    for h in range(H - 1, -1, -1):\\n        q_values = (P * (R + V[h + 1])).sum(-1)\\n        policy[h] = q_values.argmax(1)\\n        V[h] = q_values[np.arange(S), policy[h]]\\n\\n    return V, policy\";\n",
       "                var nbb_formatted_code = \"def evaluate_policy(P, R, H, policy):\\n    \\\"\\\"\\\"\\n        Parameters:\\n            P: transition function (S,A,S)-dim matrix\\n            R: reward function (S,A,S)-dim matrix\\n            H: horizon\\n            policy: a deterministic policy (H, S)-dim matrix\\n            \\n        Returns:\\n            The V-function of the provided policy\\n    \\\"\\\"\\\"\\n    S, A = P.shape[0], P.shape[1]\\n    V = np.zeros((H + 1, S))\\n\\n    for h in range(H - 1, -1, -1):\\n        pi_h = policy[h]\\n        V[h] = np.sum(\\n            P[np.arange(S), pi_h] * (R[np.arange(S), pi_h] + V[h + 1]), axis=1\\n        )\\n\\n    return V\\n\\n\\ndef backward_induction(P, R, H):\\n    \\\"\\\"\\\"\\n        Parameters:\\n            P: transition function (S,A,S)-dim matrix\\n            R: reward function (S,A,S)-dim matrix\\n            H: horizon\\n            \\n        Returns:\\n            The optimal V-function\\n            The optimal policy\\n    \\\"\\\"\\\"\\n    S, A = P.shape[0], P.shape[1]\\n    V = np.zeros((H + 1, S))\\n    policy = np.zeros((H, S), dtype=int)\\n\\n    for h in range(H - 1, -1, -1):\\n        q_values = (P * (R + V[h + 1])).sum(-1)\\n        policy[h] = q_values.argmax(1)\\n        V[h] = q_values[np.arange(S), policy[h]]\\n\\n    return V, policy\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_policy(P, R, H, policy):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            P: transition function (S,A,S)-dim matrix\n",
    "            R: reward function (S,A,S)-dim matrix\n",
    "            H: horizon\n",
    "            policy: a deterministic policy (H, S)-dim matrix\n",
    "            \n",
    "        Returns:\n",
    "            The V-function of the provided policy\n",
    "    \"\"\"\n",
    "    S, A = P.shape[0], P.shape[1]\n",
    "    V = np.zeros((H + 1, S))\n",
    "\n",
    "    for h in range(H - 1, -1, -1):\n",
    "        pi_h = policy[h]\n",
    "        V[h] = np.sum(\n",
    "            P[np.arange(S), pi_h] * (R[np.arange(S), pi_h] + V[h + 1]), axis=1\n",
    "        )\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "def backward_induction(P, R, H):\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            P: transition function (S,A,S)-dim matrix\n",
    "            R: reward function (S,A,S)-dim matrix\n",
    "            H: horizon\n",
    "            \n",
    "        Returns:\n",
    "            The optimal V-function\n",
    "            The optimal policy\n",
    "    \"\"\"\n",
    "    S, A = P.shape[0], P.shape[1]\n",
    "    V = np.zeros((H + 1, S))\n",
    "    policy = np.zeros((H, S), dtype=int)\n",
    "\n",
    "    for h in range(H - 1, -1, -1):\n",
    "        q_values = (P * (R + V[h + 1])).sum(-1)\n",
    "        policy[h] = q_values.argmax(1)\n",
    "        V[h] = q_values[np.arange(S), policy[h]]\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1_l6lqhDbYmQ"
   },
   "source": [
    "Let's set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DkOs-0y_bd61",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|\u001b[43mS\u001b[0m: : : |\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 89;\n",
       "                var nbb_unformatted_code = \"grid1 = [[\\\"\\\", \\\"\\\", \\\"\\\", \\\"g\\\"], [\\\"\\\", \\\"x\\\", \\\"\\\", \\\"\\\"], [\\\"s\\\", \\\"\\\", \\\"\\\", \\\"\\\"]]\\ngrid1_MAP = [\\n    \\\"+-------+\\\",\\n    \\\"| : : :G|\\\",\\n    \\\"| :x: : |\\\",\\n    \\\"|S: : : |\\\",\\n    \\\"+-------+\\\",\\n]\\n\\n\\nenv = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)\\nH = 6\\nenv.render()\";\n",
       "                var nbb_formatted_code = \"grid1 = [[\\\"\\\", \\\"\\\", \\\"\\\", \\\"g\\\"], [\\\"\\\", \\\"x\\\", \\\"\\\", \\\"\\\"], [\\\"s\\\", \\\"\\\", \\\"\\\", \\\"\\\"]]\\ngrid1_MAP = [\\n    \\\"+-------+\\\",\\n    \\\"| : : :G|\\\",\\n    \\\"| :x: : |\\\",\\n    \\\"|S: : : |\\\",\\n    \\\"+-------+\\\",\\n]\\n\\n\\nenv = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)\\nH = 6\\nenv.render()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid1 = [[\"\", \"\", \"\", \"g\"], [\"\", \"x\", \"\", \"\"], [\"s\", \"\", \"\", \"\"]]\n",
    "grid1_MAP = [\n",
    "    \"+-------+\",\n",
    "    \"| : : :G|\",\n",
    "    \"| :x: : |\",\n",
    "    \"|S: : : |\",\n",
    "    \"+-------+\",\n",
    "]\n",
    "\n",
    "\n",
    "env = GridWorldWithPits(grid=grid1, txt_map=grid1_MAP, uniform_trans_proba=0)\n",
    "H = 6\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Paggla09bl6S"
   },
   "source": [
    "We should test previous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tgeanS2NbpQg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.98342174 3.98668316 3.99888469 3.99900565 3.93845103 3.38442821\n",
      "  3.98743386 3.99907706 3.91567934 3.95432206 3.99516932 3.99818051]\n",
      " [3.35518231 3.38566532 3.39841218 3.39933889 3.31201648 2.75064778\n",
      "  3.38668694 3.39869023 2.99900565 3.32685495 3.37162833 3.39692963]\n",
      " [2.73311111 2.762375   2.79823611 2.79966667 2.38934167 2.144975\n",
      "  2.76311944 2.79840833 2.39933889 2.38934167 2.74873056 2.77888611]\n",
      " [1.79966667 2.15083333 2.186      2.2        1.78983333 1.19\n",
      "  2.15733333 2.18616667 1.79966667 1.78983333 1.79966667 2.161     ]\n",
      " [1.2        1.19       1.58       1.6        1.19       0.6\n",
      "  1.19       1.58       1.2        1.19       1.2        1.2       ]\n",
      " [0.6        0.6        0.6        1.         0.6        0.\n",
      "  0.6        0.6        0.6        0.6        0.6        0.6       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "[[3.98342174 3.98668316 3.99888469 3.99900565 3.93845103 3.38442821\n",
      "  3.98743386 3.99907706 3.91567934 3.95432206 3.99516932 3.99818051]\n",
      " [3.35518231 3.38566532 3.39841218 3.39933889 3.31201648 2.75064778\n",
      "  3.38668694 3.39869023 2.99900565 3.32685495 3.37162833 3.39692963]\n",
      " [2.73311111 2.762375   2.79823611 2.79966667 2.38934167 2.144975\n",
      "  2.76311944 2.79840833 2.39933889 2.38934167 2.74873056 2.77888611]\n",
      " [1.79966667 2.15083333 2.186      2.2        1.78983333 1.19\n",
      "  2.15733333 2.18616667 1.79966667 1.78983333 1.79966667 2.161     ]\n",
      " [1.2        1.19       1.58       1.6        1.19       0.6\n",
      "  1.19       1.58       1.2        1.19       1.2        1.2       ]\n",
      " [0.6        0.6        0.6        1.         0.6        0.\n",
      "  0.6        0.6        0.6        0.6        0.6        0.6       ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 90;\n",
       "                var nbb_unformatted_code = \"V, optimal_pol = backward_induction(env.P, env.R, H)\\nprint(V)\\nVpi = evaluate_policy(env.P, env.R, H, optimal_pol)\\nprint(Vpi)\\nassert np.allclose(V, Vpi)\";\n",
       "                var nbb_formatted_code = \"V, optimal_pol = backward_induction(env.P, env.R, H)\\nprint(V)\\nVpi = evaluate_policy(env.P, env.R, H, optimal_pol)\\nprint(Vpi)\\nassert np.allclose(V, Vpi)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "V, optimal_pol = backward_induction(env.P, env.R, H)\n",
    "print(V)\n",
    "Vpi = evaluate_policy(env.P, env.R, H, optimal_pol)\n",
    "print(Vpi)\n",
    "assert np.allclose(V, Vpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MACThGeMb48-"
   },
   "source": [
    "Run the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vbyoa9Qpb6Yb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|\u001b[43mS\u001b[0m: : : |\n",
      "+-------+\n",
      "\n",
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|S:\u001b[43m_\u001b[0m: : |\n",
      "+-------+\n",
      "  (right)\n",
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|S: :\u001b[43m_\u001b[0m: |\n",
      "+-------+\n",
      "  (right)\n",
      "+-------+\n",
      "| : : :G|\n",
      "| :x: : |\n",
      "|S: : :\u001b[43m_\u001b[0m|\n",
      "+-------+\n",
      "  (right)\n",
      "+-------+\n",
      "| : : :G|\n",
      "| :x: :\u001b[43m_\u001b[0m|\n",
      "|S: : : |\n",
      "+-------+\n",
      "  (up)\n",
      "+-------+\n",
      "| : : :\u001b[42mG\u001b[0m|\n",
      "| :x: : |\n",
      "|S: : : |\n",
      "+-------+\n",
      "  (up)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 91;\n",
       "                var nbb_unformatted_code = \"state = env.reset()\\nenv.render()\\nfor i in range(H):\\n    next_state, reward, done, _ = env.step(optimal_pol[i, state])\\n    env.render()\\n    state = next_state\\n    if done:\\n        break\";\n",
       "                var nbb_formatted_code = \"state = env.reset()\\nenv.render()\\nfor i in range(H):\\n    next_state, reward, done, _ = env.step(optimal_pol[i, state])\\n    env.render()\\n    state = next_state\\n    if done:\\n        break\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "env.render()\n",
    "for i in range(H):\n",
    "    next_state, reward, done, _ = env.step(optimal_pol[i, state])\n",
    "    env.render()\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RZtdzpwUcHYj"
   },
   "source": [
    "Finally we are ready to implement our exploration algorithm.\n",
    "\n",
    "**Question 2**: implement the **UCB-VI** algorithm.\n",
    "\n",
    "UCBVI is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
    "In this setting, the regret is defined as\n",
    "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
    "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
    "\n",
    "The structure of the algorithm is as follow\n",
    "\n",
    "For $k = 1, \\ldots, K$ do<br>\n",
    "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
    "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
    ">> for $h=1, \\ldots, H$<br>\n",
    ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
    ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
    ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
    "\n",
    "<font color='#ed7d31'>Optimistic planning</font>\n",
    "\n",
    "UCBVI exploits exploration bonus to perform optimistic planning on the empirical MDP $(\\hat{p}, \\hat{r})$.\n",
    "The optimal Q-function of this MDP can be obtained using backward induction.\n",
    "\n",
    "The optimal Bellman operator for Q-function is defined similarly as\n",
    "$$\n",
    "Q_h^\\star(s,a) =  b(s,a) + \\sum_{s'} p(s'|s,a) \\left( r(s,a,s') + \\max_{a'} Q_{h+1}^\\star(s',a')\\right) \n",
    "$$\n",
    "where $Q_{H+1}(s,a) = 0$ and $b$ is an exploration bonus.\n",
    "\n",
    "<font color='#ed7d31'>Exploration Bonus</font>\n",
    "\n",
    "Using Hoeffding's bound we have that\n",
    "$$\n",
    "b_{k,h}(s,a) = 7(H-h+1)L\\sqrt{\\frac{1}{N_k(s,a)}}\n",
    "$$\n",
    "where $L = \\ln(5SAT/\\delta)$.\n",
    "\n",
    "A tighter exploration bonus is obtained using Bernstein's bound. Since it's expression is much more complicated, we provided the code (see `compute_bernstein_bonus`).\n",
    "\n",
    "\n",
    "Refer to [Minimax Regret Bounds for Reinforcement Learning](https://arxiv.org/abs/1703.05449) for additional details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXMJTmLPcOwS"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 138;\n",
       "                var nbb_unformatted_code = \"class UCBVI:\\n    def __init__(self, config):\\n        np.random.seed(seed=config[\\\"seed\\\"])\\n        self.env = config[\\\"env\\\"]\\n        self.horizon = config[\\\"horizon\\\"]\\n        self.scale_factor = config[\\\"scale_factor\\\"]\\n        self.nb_repetitions = config[\\\"repetitions\\\"]\\n        self.nb_episodes = config[\\\"episodes\\\"]\\n        assert config[\\\"b_type\\\"] in [\\\"hoeffding\\\", \\\"bernstein\\\"]\\n        self.b_type = config[\\\"b_type\\\"]\\n        self.reset()\\n\\n    def reset(self):\\n        S, A = self.env.Ns, self.env.Na\\n        self.delta = 0.1\\n        self.t = 0\\n        self.episode = 0\\n        self.Phat = np.zeros((S, A, S))\\n        self.Rhat = np.zeros((S, A, S))\\n        self.N_sa = np.zeros((S, A), dtype=np.int)\\n        self.N_sas = np.zeros((S, A, S), dtype=np.int)\\n        self.policy = np.zeros((self.horizon, S), dtype=np.int)\\n        self.V = np.zeros((self.horizon + 1, S))\\n        self.Q = np.zeros((self.horizon + 1, S, A))\\n        self.bonus = np.zeros((self.horizon, S, A))\\n\\n    def get_optimistic_q(self):\\n        \\\"\\\"\\\" Compute optimistic Q-function and associated greedy policy\\n        \\\"\\\"\\\"\\n        H = self.horizon\\n        S, A = self.N_sa.shape\\n        self.V.fill(0)\\n        self.Q.fill(0)\\n\\n        if self.b_type == \\\"hoeffding\\\":\\n            self.compute_hoeffding_bonus()\\n\\n        for h in reversed(range(H)):\\n            if self.b_type == \\\"bernstein\\\":\\n                self.compute_bernstein_bonus(h, self.V[h + 1])\\n\\n            self.Q[h] = self.bonus[h] + np.sum(\\n                self.Phat * (self.Rhat + self.Q[h + 1].max(1)), axis=-1\\n            )\\n            self.policy[h] = self.Q[h].argmax(1)\\n            self.V[h] = self.Q[h, np.arange(S), self.policy[h]]\\n            self.V[h] = np.minimum(H - h + 2.0, self.V[h])\\n\\n    def compute_hoeffding_bonus(self):\\n        \\\"\\\"\\\"Compute Hoeffding-based exploration bonus\\n        \\\"\\\"\\\"\\n        S, A = self.env.Ns, self.env.Na\\n        N = np.maximum(1.0, self.N_sa)\\n        #         L = np.log(5 * S * A * max(self.t, 1) / self.delta)\\n        L = np.log(5.0 * S * A * N / self.delta)\\n\\n        for h in range(self.horizon):\\n            self.bonus[h] = (\\n                self.scale_factor * 7.0 * (self.horizon - h + 2.0) * np.sqrt(L / N)\\n            )\\n\\n    def compute_bernstein_bonus(self, h, Vhp1):\\n        \\\"\\\"\\\"Compute Bernstein-based exploration bonus\\n\\n        Parameters:\\n            h: state\\n            Vhp1: value function at state h+1 (S-dim vector)\\n        \\\"\\\"\\\"\\n        S, A = self.N_sa.shape\\n        for s in range(S):\\n            for a in range(A):\\n                L = np.log(5 * S * A * max(1, self.N_sa[s][a]) / self.delta)\\n                n = max(1, self.N_sa[s][a])\\n                var, mean = 0, 0\\n                for i in range(S):\\n                    mean += self.Phat[s, a, i] * Vhp1[i]\\n                for i in range(S):\\n                    var += self.Phat[s, a, i] * (Vhp1[i] - mean) * (Vhp1[i] - mean)\\n                T1 = np.sqrt(8 * L * var / n) + 14 * L * (self.horizon - h + 2) / (\\n                    3 * n\\n                )\\n                T2 = np.sqrt(8 * (self.horizon - h + 2) ** 2 / n)\\n                self.bonus[h, s, a] = self.scale_factor * (T1 + T2)\\n\\n    def update(self, state, action, reward, next_state):\\n        \\\"\\\"\\\"Update the internal statistics\\n        \\\"\\\"\\\"\\n        self.N_sas[state, action, next_state] += 1\\n        self.N_sa[state, action] += 1\\n        self.Phat[state, action, next_state] = (\\n            self.N_sas[state, action, next_state] / self.N_sa[state, action]\\n        )\\n        self.Rhat[state, action, next_state] = (\\n            (self.N_sas[state, action, next_state] - 1)\\n            * self.Rhat[state, action, next_state]\\n            + reward\\n        ) / self.N_sas[state, action, next_state]\\n\\n    def run_episode(self):\\n        episode_reward = 0\\n        state = self.env.reset()\\n        initial_state = state\\n        self.get_optimistic_q()\\n\\n        Vpi = evaluate_policy(self.env.P, self.env.R, self.horizon, self.policy)\\n        self.episode_value.append(Vpi[0, initial_state])\\n\\n        for h in range(self.horizon):\\n            action = self.policy[h, state]\\n            next_state, reward, done, info = self.env.step(action)\\n            self.update(state, action, reward, next_state)\\n            episode_reward += reward\\n            state = next_state\\n            self.t += 1\\n        self.episode += 1\\n        return initial_state, Vpi\\n\\n    def train(self):\\n        # compute true value function (for the regret)\\n        trueV, _ = backward_induction(self.env.P, self.env.R, self.horizon)\\n        regret = np.zeros((self.nb_repetitions, self.nb_episodes + 1))\\n        self.episode_value = []\\n\\n        for rep in range(self.nb_repetitions):\\n            self.reset()\\n            old_regret = 0\\n            for k in tqdm(range(self.nb_episodes)):\\n                init_state, Vpi = self.run_episode()\\n                episode_regret = trueV[0, init_state] - Vpi[0, init_state]\\n                old_regret += episode_regret\\n                regret[rep, k + 1] = old_regret\\n        return regret\";\n",
       "                var nbb_formatted_code = \"class UCBVI:\\n    def __init__(self, config):\\n        np.random.seed(seed=config[\\\"seed\\\"])\\n        self.env = config[\\\"env\\\"]\\n        self.horizon = config[\\\"horizon\\\"]\\n        self.scale_factor = config[\\\"scale_factor\\\"]\\n        self.nb_repetitions = config[\\\"repetitions\\\"]\\n        self.nb_episodes = config[\\\"episodes\\\"]\\n        assert config[\\\"b_type\\\"] in [\\\"hoeffding\\\", \\\"bernstein\\\"]\\n        self.b_type = config[\\\"b_type\\\"]\\n        self.reset()\\n\\n    def reset(self):\\n        S, A = self.env.Ns, self.env.Na\\n        self.delta = 0.1\\n        self.t = 0\\n        self.episode = 0\\n        self.Phat = np.zeros((S, A, S))\\n        self.Rhat = np.zeros((S, A, S))\\n        self.N_sa = np.zeros((S, A), dtype=np.int)\\n        self.N_sas = np.zeros((S, A, S), dtype=np.int)\\n        self.policy = np.zeros((self.horizon, S), dtype=np.int)\\n        self.V = np.zeros((self.horizon + 1, S))\\n        self.Q = np.zeros((self.horizon + 1, S, A))\\n        self.bonus = np.zeros((self.horizon, S, A))\\n\\n    def get_optimistic_q(self):\\n        \\\"\\\"\\\" Compute optimistic Q-function and associated greedy policy\\n        \\\"\\\"\\\"\\n        H = self.horizon\\n        S, A = self.N_sa.shape\\n        self.V.fill(0)\\n        self.Q.fill(0)\\n\\n        if self.b_type == \\\"hoeffding\\\":\\n            self.compute_hoeffding_bonus()\\n\\n        for h in reversed(range(H)):\\n            if self.b_type == \\\"bernstein\\\":\\n                self.compute_bernstein_bonus(h, self.V[h + 1])\\n\\n            self.Q[h] = self.bonus[h] + np.sum(\\n                self.Phat * (self.Rhat + self.Q[h + 1].max(1)), axis=-1\\n            )\\n            self.policy[h] = self.Q[h].argmax(1)\\n            self.V[h] = self.Q[h, np.arange(S), self.policy[h]]\\n            self.V[h] = np.minimum(H - h + 2.0, self.V[h])\\n\\n    def compute_hoeffding_bonus(self):\\n        \\\"\\\"\\\"Compute Hoeffding-based exploration bonus\\n        \\\"\\\"\\\"\\n        S, A = self.env.Ns, self.env.Na\\n        N = np.maximum(1.0, self.N_sa)\\n        #         L = np.log(5 * S * A * max(self.t, 1) / self.delta)\\n        L = np.log(5.0 * S * A * N / self.delta)\\n\\n        for h in range(self.horizon):\\n            self.bonus[h] = (\\n                self.scale_factor * 7.0 * (self.horizon - h + 2.0) * np.sqrt(L / N)\\n            )\\n\\n    def compute_bernstein_bonus(self, h, Vhp1):\\n        \\\"\\\"\\\"Compute Bernstein-based exploration bonus\\n\\n        Parameters:\\n            h: state\\n            Vhp1: value function at state h+1 (S-dim vector)\\n        \\\"\\\"\\\"\\n        S, A = self.N_sa.shape\\n        for s in range(S):\\n            for a in range(A):\\n                L = np.log(5 * S * A * max(1, self.N_sa[s][a]) / self.delta)\\n                n = max(1, self.N_sa[s][a])\\n                var, mean = 0, 0\\n                for i in range(S):\\n                    mean += self.Phat[s, a, i] * Vhp1[i]\\n                for i in range(S):\\n                    var += self.Phat[s, a, i] * (Vhp1[i] - mean) * (Vhp1[i] - mean)\\n                T1 = np.sqrt(8 * L * var / n) + 14 * L * (self.horizon - h + 2) / (\\n                    3 * n\\n                )\\n                T2 = np.sqrt(8 * (self.horizon - h + 2) ** 2 / n)\\n                self.bonus[h, s, a] = self.scale_factor * (T1 + T2)\\n\\n    def update(self, state, action, reward, next_state):\\n        \\\"\\\"\\\"Update the internal statistics\\n        \\\"\\\"\\\"\\n        self.N_sas[state, action, next_state] += 1\\n        self.N_sa[state, action] += 1\\n        self.Phat[state, action, next_state] = (\\n            self.N_sas[state, action, next_state] / self.N_sa[state, action]\\n        )\\n        self.Rhat[state, action, next_state] = (\\n            (self.N_sas[state, action, next_state] - 1)\\n            * self.Rhat[state, action, next_state]\\n            + reward\\n        ) / self.N_sas[state, action, next_state]\\n\\n    def run_episode(self):\\n        episode_reward = 0\\n        state = self.env.reset()\\n        initial_state = state\\n        self.get_optimistic_q()\\n\\n        Vpi = evaluate_policy(self.env.P, self.env.R, self.horizon, self.policy)\\n        self.episode_value.append(Vpi[0, initial_state])\\n\\n        for h in range(self.horizon):\\n            action = self.policy[h, state]\\n            next_state, reward, done, info = self.env.step(action)\\n            self.update(state, action, reward, next_state)\\n            episode_reward += reward\\n            state = next_state\\n            self.t += 1\\n        self.episode += 1\\n        return initial_state, Vpi\\n\\n    def train(self):\\n        # compute true value function (for the regret)\\n        trueV, _ = backward_induction(self.env.P, self.env.R, self.horizon)\\n        regret = np.zeros((self.nb_repetitions, self.nb_episodes + 1))\\n        self.episode_value = []\\n\\n        for rep in range(self.nb_repetitions):\\n            self.reset()\\n            old_regret = 0\\n            for k in tqdm(range(self.nb_episodes)):\\n                init_state, Vpi = self.run_episode()\\n                episode_regret = trueV[0, init_state] - Vpi[0, init_state]\\n                old_regret += episode_regret\\n                regret[rep, k + 1] = old_regret\\n        return regret\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class UCBVI:\n",
    "    def __init__(self, config):\n",
    "        np.random.seed(seed=config[\"seed\"])\n",
    "        self.env = config[\"env\"]\n",
    "        self.horizon = config[\"horizon\"]\n",
    "        self.scale_factor = config[\"scale_factor\"]\n",
    "        self.nb_repetitions = config[\"repetitions\"]\n",
    "        self.nb_episodes = config[\"episodes\"]\n",
    "        assert config[\"b_type\"] in [\"hoeffding\", \"bernstein\"]\n",
    "        self.b_type = config[\"b_type\"]\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        S, A = self.env.Ns, self.env.Na\n",
    "        self.delta = 0.1\n",
    "        self.t = 0\n",
    "        self.episode = 0\n",
    "        self.Phat = np.zeros((S, A, S))\n",
    "        self.Rhat = np.zeros((S, A, S))\n",
    "        self.N_sa = np.zeros((S, A), dtype=np.int)\n",
    "        self.N_sas = np.zeros((S, A, S), dtype=np.int)\n",
    "        self.policy = np.zeros((self.horizon, S), dtype=np.int)\n",
    "        self.V = np.zeros((self.horizon + 1, S))\n",
    "        self.Q = np.zeros((self.horizon + 1, S, A))\n",
    "        self.bonus = np.zeros((self.horizon, S, A))\n",
    "\n",
    "    def get_optimistic_q(self):\n",
    "        \"\"\" Compute optimistic Q-function and associated greedy policy\n",
    "        \"\"\"\n",
    "        H = self.horizon\n",
    "        S, A = self.N_sa.shape\n",
    "        self.V.fill(0)\n",
    "        self.Q.fill(0)\n",
    "\n",
    "        if self.b_type == \"hoeffding\":\n",
    "            self.compute_hoeffding_bonus()\n",
    "\n",
    "        for h in reversed(range(H)):\n",
    "            if self.b_type == \"bernstein\":\n",
    "                self.compute_bernstein_bonus(h, self.V[h + 1])\n",
    "\n",
    "            self.Q[h] = self.bonus[h] + np.sum(\n",
    "                self.Phat * (self.Rhat + self.Q[h + 1].max(1)), axis=-1\n",
    "            )\n",
    "            self.policy[h] = self.Q[h].argmax(1)\n",
    "            self.V[h] = self.Q[h, np.arange(S), self.policy[h]]\n",
    "            self.V[h] = np.minimum(H - h + 2.0, self.V[h])\n",
    "\n",
    "    def compute_hoeffding_bonus(self):\n",
    "        \"\"\"Compute Hoeffding-based exploration bonus\n",
    "        \"\"\"\n",
    "        S, A = self.env.Ns, self.env.Na\n",
    "        N = np.maximum(1.0, self.N_sa)\n",
    "        # L = np.log(5 * S * A * max(self.t, 1) / self.delta)\n",
    "        L = np.log(5.0 * S * A * N / self.delta)\n",
    "\n",
    "        for h in range(self.horizon):\n",
    "            self.bonus[h] = (\n",
    "                self.scale_factor * 7.0 * (self.horizon - h + 2.0) * np.sqrt(L / N)\n",
    "            )\n",
    "\n",
    "    def compute_bernstein_bonus(self, h, Vhp1):\n",
    "        \"\"\"Compute Bernstein-based exploration bonus\n",
    "\n",
    "        Parameters:\n",
    "            h: state\n",
    "            Vhp1: value function at state h+1 (S-dim vector)\n",
    "        \"\"\"\n",
    "        S, A = self.N_sa.shape\n",
    "        for s in range(S):\n",
    "            for a in range(A):\n",
    "                L = np.log(5 * S * A * max(1, self.N_sa[s][a]) / self.delta)\n",
    "                n = max(1, self.N_sa[s][a])\n",
    "                var, mean = 0, 0\n",
    "                for i in range(S):\n",
    "                    mean += self.Phat[s, a, i] * Vhp1[i]\n",
    "                for i in range(S):\n",
    "                    var += self.Phat[s, a, i] * (Vhp1[i] - mean) * (Vhp1[i] - mean)\n",
    "                T1 = np.sqrt(8 * L * var / n) + 14 * L * (self.horizon - h + 2) / (\n",
    "                    3 * n\n",
    "                )\n",
    "                T2 = np.sqrt(8 * (self.horizon - h + 2) ** 2 / n)\n",
    "                self.bonus[h, s, a] = self.scale_factor * (T1 + T2)\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"Update the internal statistics\n",
    "        \"\"\"\n",
    "        self.N_sas[state, action, next_state] += 1\n",
    "        self.N_sa[state, action] += 1\n",
    "        self.Phat[state, action, next_state] = (\n",
    "            self.N_sas[state, action, next_state] / self.N_sa[state, action]\n",
    "        )\n",
    "        self.Rhat[state, action, next_state] = (\n",
    "            (self.N_sas[state, action, next_state] - 1)\n",
    "            * self.Rhat[state, action, next_state]\n",
    "            + reward\n",
    "        ) / self.N_sas[state, action, next_state]\n",
    "\n",
    "    def run_episode(self):\n",
    "        episode_reward = 0\n",
    "        state = self.env.reset()\n",
    "        initial_state = state\n",
    "        self.get_optimistic_q()\n",
    "\n",
    "        Vpi = evaluate_policy(self.env.P, self.env.R, self.horizon, self.policy)\n",
    "        self.episode_value.append(Vpi[0, initial_state])\n",
    "\n",
    "        for h in range(self.horizon):\n",
    "            action = self.policy[h, state]\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            self.update(state, action, reward, next_state)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            self.t += 1\n",
    "        self.episode += 1\n",
    "        return initial_state, Vpi\n",
    "\n",
    "    def train(self):\n",
    "        # compute true value function (for the regret)\n",
    "        trueV, _ = backward_induction(self.env.P, self.env.R, self.horizon)\n",
    "        regret = np.zeros((self.nb_repetitions, self.nb_episodes + 1))\n",
    "        self.episode_value = []\n",
    "\n",
    "        for rep in range(self.nb_repetitions):\n",
    "            self.reset()\n",
    "            old_regret = 0\n",
    "            for k in tqdm(range(self.nb_episodes)):\n",
    "                init_state, Vpi = self.run_episode()\n",
    "                episode_regret = trueV[0, init_state] - Vpi[0, init_state]\n",
    "                old_regret += episode_regret\n",
    "                regret[rep, k + 1] = old_regret\n",
    "        return regret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MoSaZ3xOckmd"
   },
   "source": [
    "Define the settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fjk1TbBock-G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current config is:\n",
      "{'b_type': 'hoeffding',\n",
      " 'env': <gridworld.GridWorldWithPits object at 0x7f1135c4aa20>,\n",
      " 'episodes': 15000,\n",
      " 'horizon': 6,\n",
      " 'repetitions': 6,\n",
      " 'scale_factor': 0.1,\n",
      " 'seed': 14}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 141;\n",
       "                var nbb_unformatted_code = \"config = {\\n    \\\"env\\\": env,\\n    \\\"scale_factor\\\": 0.1,  # we use a scaling factor in order to increase the convergence speed\\n    \\\"seed\\\": 14,\\n    \\\"horizon\\\": H,\\n    \\\"episodes\\\": 15000,\\n    \\\"repetitions\\\": 6,\\n    \\\"b_type\\\": \\\"hoeffding\\\",  # [hoeffding, bernstein]\\n}\\n\\nprint(\\\"Current config is:\\\")\\npprint(config)\";\n",
       "                var nbb_formatted_code = \"config = {\\n    \\\"env\\\": env,\\n    \\\"scale_factor\\\": 0.1,  # we use a scaling factor in order to increase the convergence speed\\n    \\\"seed\\\": 14,\\n    \\\"horizon\\\": H,\\n    \\\"episodes\\\": 15000,\\n    \\\"repetitions\\\": 6,\\n    \\\"b_type\\\": \\\"hoeffding\\\",  # [hoeffding, bernstein]\\n}\\n\\nprint(\\\"Current config is:\\\")\\npprint(config)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    \"env\": env,\n",
    "    \"scale_factor\": 0.1,  # we use a scaling factor in order to increase the convergence speed\n",
    "    \"seed\": 14,\n",
    "    \"horizon\": H,\n",
    "    \"episodes\": 15000,\n",
    "    \"repetitions\": 6,\n",
    "    \"b_type\": \"hoeffding\",  # [hoeffding, bernstein]\n",
    "}\n",
    "\n",
    "print(\"Current config is:\")\n",
    "pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSI7NR50cyFx"
   },
   "source": [
    "Run the agent and compare the behaviour with Hoeffding and Bernstein bound.\n",
    "\n",
    "A picture is automatically generated (it will show the regret average regret of the two algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8S3ObC9ncydR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:07<00:00, 2095.36it/s]\n",
      "100%|██████████| 15000/15000 [00:07<00:00, 2093.75it/s]\n",
      "100%|██████████| 15000/15000 [00:06<00:00, 2158.84it/s]\n",
      "100%|██████████| 15000/15000 [00:07<00:00, 2124.15it/s]\n",
      "100%|██████████| 15000/15000 [00:07<00:00, 2115.15it/s]\n",
      "100%|██████████| 15000/15000 [00:07<00:00, 2117.98it/s]\n",
      "100%|██████████| 15000/15000 [01:28<00:00, 170.27it/s]\n",
      "100%|██████████| 15000/15000 [01:28<00:00, 169.35it/s]\n",
      "100%|██████████| 15000/15000 [01:28<00:00, 168.77it/s]\n",
      "100%|██████████| 15000/15000 [01:27<00:00, 170.49it/s]\n",
      "100%|██████████| 15000/15000 [01:28<00:00, 170.12it/s]\n",
      "100%|██████████| 15000/15000 [01:27<00:00, 172.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f11321174a8>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 142;\n",
       "                var nbb_unformatted_code = \"plt.figure(figsize=(10, 8))\\nregret = {}\\nfor bound in [\\\"hoeffding\\\", \\\"bernstein\\\"]:\\n    tmp_config = copy.copy(config)\\n    tmp_config[\\\"b_type\\\"] = bound\\n    agent = UCBVI(config=tmp_config)\\n    regret[bound] = agent.train()\\n\\n    mean_regret = np.mean(regret[bound], axis=0)\\n    std = np.std(regret[bound], axis=0) / np.sqrt(regret[bound].shape[0])\\n    x = np.arange(regret[bound].shape[1])\\n    plt.plot(x, mean_regret, label=bound)\\n    plt.fill_between(x, mean_regret + 2 * std, mean_regret - 2 * std, alpha=0.15)\\nplt.legend()\";\n",
       "                var nbb_formatted_code = \"plt.figure(figsize=(10, 8))\\nregret = {}\\nfor bound in [\\\"hoeffding\\\", \\\"bernstein\\\"]:\\n    tmp_config = copy.copy(config)\\n    tmp_config[\\\"b_type\\\"] = bound\\n    agent = UCBVI(config=tmp_config)\\n    regret[bound] = agent.train()\\n\\n    mean_regret = np.mean(regret[bound], axis=0)\\n    std = np.std(regret[bound], axis=0) / np.sqrt(regret[bound].shape[0])\\n    x = np.arange(regret[bound].shape[1])\\n    plt.plot(x, mean_regret, label=bound)\\n    plt.fill_between(x, mean_regret + 2 * std, mean_regret - 2 * std, alpha=0.15)\\nplt.legend()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "regret = {}\n",
    "for bound in [\"hoeffding\", \"bernstein\"]:\n",
    "    tmp_config = copy.copy(config)\n",
    "    tmp_config[\"b_type\"] = bound\n",
    "    agent = UCBVI(config=tmp_config)\n",
    "    regret[bound] = agent.train()\n",
    "\n",
    "    mean_regret = np.mean(regret[bound], axis=0)\n",
    "    std = np.std(regret[bound], axis=0) / np.sqrt(regret[bound].shape[0])\n",
    "    x = np.arange(regret[bound].shape[1])\n",
    "    plt.plot(x, mean_regret, label=bound)\n",
    "    plt.fill_between(x, mean_regret + 2 * std, mean_regret - 2 * std, alpha=0.15)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MVARL19_part4.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
